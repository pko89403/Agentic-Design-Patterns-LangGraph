{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67024ad8",
   "metadata": {},
   "source": [
    "# Chapter 8: Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d795f2c",
   "metadata": {},
   "source": [
    "ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ê°€ ì •ë³´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” **íš¨ê³¼ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬**ê°€ í•„ìˆ˜ì ì´ë‹¤.  \n",
    "ì—ì´ì „íŠ¸ëŠ” ì¸ê°„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ íš¨ìœ¨ì ì¸ ì‘ë™ì„ ìœ„í•´ **ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ë©”ëª¨ë¦¬**ë¥¼ í•„ìš”ë¡œ í•œë‹¤.  \n",
    "ì´ ì¥ì—ì„œëŠ” íŠ¹íˆ ì—ì´ì „íŠ¸ì˜ **ì¦‰ê°ì (ë‹¨ê¸°)** ë° **ì§€ì†ì (ì¥ê¸°)** ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ë‹¤ë£¬ë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ëŠ” ê³¼ê±°ì˜ **ìƒí˜¸ì‘ìš©, ê´€ì°°, í•™ìŠµ ê²½í—˜**ì—ì„œ ì–»ì€ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  í™œìš©í•˜ëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•œë‹¤.  \n",
    "ì´ ê¸°ëŠ¥ì€ ì—ì´ì „íŠ¸ê°€ **ì •ë³´ì— ê¸°ë°˜í•œ ì˜ì‚¬ê²°ì •**ì„ ë‚´ë¦¬ê³ , **ëŒ€í™” ë§¥ë½ì„ ìœ ì§€**í•˜ë©°, **ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¤ë„ë¡ ë•ëŠ”ë‹¤.\n",
    "\n",
    "ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‘ ê°€ì§€ ì£¼ìš” ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ee3e7",
   "metadata": {},
   "source": [
    "## 1. ë‹¨ê¸° ë©”ëª¨ë¦¬ (Short-Term Memory, Contextual Memory)\n",
    "\n",
    "**ì‘ì—… ê¸°ì–µ(working memory)**ê³¼ ìœ ì‚¬í•˜ê²Œ, í˜„ì¬ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜ ìµœê·¼ì— ì ‘ê·¼í•œ ì •ë³´ë¥¼ ë³´ìœ í•œë‹¤.  \n",
    "LLMì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ì˜ ê²½ìš°, ë‹¨ê¸° ë©”ëª¨ë¦¬ëŠ” ì£¼ë¡œ **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°(context window)** ë‚´ì— ì¡´ì¬í•œë‹¤.\n",
    "\n",
    "ì´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ëœë‹¤:\n",
    "- ìµœê·¼ì˜ ë©”ì‹œì§€  \n",
    "- ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ  \n",
    "- ë„êµ¬ ì‚¬ìš© ê²°ê³¼  \n",
    "- í˜„ì¬ ìƒí˜¸ì‘ìš©ì—ì„œì˜ ì—ì´ì „íŠ¸ ë°˜ì„±(reflections)\n",
    "\n",
    "ì´ ëª¨ë“  ì •ë³´ëŠ” LLMì˜ ë‹¤ìŒ ì‘ë‹µê³¼ í–‰ë™ì„ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.  \n",
    "ê·¸ëŸ¬ë‚˜ ì´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì—ëŠ” **ìš©ëŸ‰ ì œí•œ**ì´ ìˆì–´, ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì´ ì œí•œëœë‹¤.\n",
    "\n",
    "íš¨ìœ¨ì ì¸ ë‹¨ê¸° ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” **ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´**ë¥¼ ì´ ì œí•œëœ ê³µê°„ì— ìœ ì§€í•˜ëŠ” ê²ƒì´ë‹¤.  \n",
    "ì´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ìˆ ì´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤:\n",
    "- ì˜¤ë˜ëœ ëŒ€í™” ë¶€ë¶„ì„ ìš”ì•½(summarization)  \n",
    "- í•µì‹¬ ì„¸ë¶€ì‚¬í•­ ê°•ì¡°(highlighting key details)\n",
    "\n",
    "ìµœê·¼ì—ëŠ” **â€˜ê¸´ ì»¨í…ìŠ¤íŠ¸(long context)â€™ ìœˆë„ìš°**ë¥¼ ê°–ì¶˜ ëª¨ë¸ì´ ë“±ì¥í•˜ì—¬  \n",
    "ë‹¨ê¸° ë©”ëª¨ë¦¬ì˜ í¬ê¸°ë¥¼ í™•ì¥í•˜ê³ , í•œ ë²ˆì˜ ìƒí˜¸ì‘ìš©ì—ì„œ ë” ë§ì€ ì •ë³´ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.  \n",
    "í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì»¨í…ìŠ¤íŠ¸ëŠ” ì—¬ì „íˆ íœ˜ë°œì„±(ephemeral)ì´ë©°, **ì„¸ì…˜ì´ ì¢…ë£Œë˜ë©´ ì‚¬ë¼ì§„ë‹¤**.  \n",
    "ë˜í•œ ë§¤ë²ˆ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ **ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ë¹„íš¨ìœ¨ì **ì¼ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì—ì´ì „íŠ¸ê°€ **ì§€ì†ì ìœ¼ë¡œ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³ **,  \n",
    "**ì´ì „ ìƒí˜¸ì‘ìš©ì˜ ì •ë³´ë¥¼ ì¬í™œìš©í•˜ë©°**, **ì§€ì†ì ì¸ ì§€ì‹ ê¸°ë°˜ì„ êµ¬ì¶•**í•˜ê¸° ìœ„í•´ì„œëŠ”  \n",
    "ë³„ë„ì˜ ì¥ê¸° ë©”ëª¨ë¦¬(persistent memory)ê°€ í•„ìš”í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06160ef9",
   "metadata": {},
   "source": [
    "## 2. ì¥ê¸° ë©”ëª¨ë¦¬ (Long-Term Memory, Persistent Memory)\n",
    "\n",
    "ì¥ê¸° ë©”ëª¨ë¦¬ëŠ” ë‹¤ì–‘í•œ ìƒí˜¸ì‘ìš©, ì‘ì—…, ì¥ê¸°ê°„ì˜ í™œë™ì—ì„œ  \n",
    "ì—ì´ì „íŠ¸ê°€ **ì§€ì†ì ìœ¼ë¡œ ë³´ìœ í•´ì•¼ í•˜ëŠ” ì •ë³´ì˜ ì €ì¥ì†Œ** ì—­í• ì„ í•œë‹¤.  \n",
    "ì´ëŠ” ì¸ê°„ì˜ ì¥ê¸° ê¸°ì–µì²˜ëŸ¼ ì‘ë™í•œë‹¤.\n",
    "\n",
    "ì´ ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì¦‰ê°ì ì¸ ì²˜ë¦¬ í™˜ê²½ ì™¸ë¶€ì— ì €ì¥ë˜ë©°,  \n",
    "ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤:\n",
    "- **ë°ì´í„°ë² ì´ìŠ¤**\n",
    "- **ì§€ì‹ ê·¸ë˜í”„(knowledge graphs)**\n",
    "- **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(vector databases)**\n",
    "\n",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œëŠ” ì •ë³´ê°€ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜ë˜ì–´ ì €ì¥ëœë‹¤.  \n",
    "ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” **ì •í™•í•œ í‚¤ì›Œë“œ ì¼ì¹˜**ê°€ ì•„ë‹Œ  \n",
    "**ì˜ë¯¸ì  ìœ ì‚¬ì„±(semantic similarity)** ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ”ë°,  \n",
    "ì´ë¥¼ semantic searchì´ë¼ê³  í•œë‹¤.\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•  ë•ŒëŠ” ë‹¤ìŒì˜ ê³¼ì •ì„ ê±°ì¹œë‹¤:\n",
    "1. ì™¸ë¶€ ì €ì¥ì†Œì— ì¿¼ë¦¬(query)ë¥¼ ë³´ë‚¸ë‹¤.  \n",
    "2. ê´€ë ¨ ë°ì´í„°ë¥¼ ê²€ìƒ‰(retrieve)í•œë‹¤.  \n",
    "3. í•´ë‹¹ ì •ë³´ë¥¼ ë‹¨ê¸° ì»¨í…ìŠ¤íŠ¸ì— í†µí•©(integrate)í•˜ì—¬ ì¦‰ì‹œ í™œìš©í•œë‹¤.\n",
    "\n",
    "ì´ ê³¼ì •ì„ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ê¸°ì¡´ ì§€ì‹(prior knowledge)ê³¼  \n",
    "í˜„ì¬ ìƒí˜¸ì‘ìš©(current interaction)ì„ ê²°í•©í•˜ì—¬ ë”ìš± ì§€ëŠ¥ì ìœ¼ë¡œ ì‘ë™í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927f027",
   "metadata": {},
   "source": [
    "# ì‹¤ìš©ì  ì‘ìš© ë° í™œìš© ì‚¬ë¡€ (Practical Applications & Use Cases)\n",
    "\n",
    "ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” ì—ì´ì „íŠ¸ê°€ ì •ë³´ë¥¼ ì¶”ì í•˜ê³ , ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ **ì§€ëŠ¥ì ìœ¼ë¡œ ì‘ë™**í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤.  \n",
    "ì´ëŠ” ë‹¨ìˆœí•œ **ì§ˆë¬¸-ì‘ë‹µ(Q&A)** ëŠ¥ë ¥ì„ ë„˜ì–´ì„œëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°,  \n",
    "ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ìš© ì‚¬ë¡€ì—ì„œ ê·¸ ì¤‘ìš”ì„±ì´ ë“œëŸ¬ë‚œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ì±—ë´‡ ë° ëŒ€í™”í˜• AI (Chatbots and Conversational AI)\n",
    "\n",
    "ë‹¨ê¸° ë©”ëª¨ë¦¬ : ëŒ€í™”ì˜ íë¦„ì„ ìœ ì§€, ì±—ë´‡ì€ ì´ì „ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ì–µí•´ì•¼ **ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ**ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤.  \n",
    "ì¥ê¸° ë©”ëª¨ë¦¬ : \n",
    "- ì‚¬ìš©ì **ì„ í˜¸ë„(preferences)** ê¸°ì–µ  \n",
    "- ê³¼ê±°ì˜ **ë¬¸ì œë‚˜ ëŒ€í™” ê¸°ë¡** íšŒìƒ  \n",
    "- **ê°œì¸í™”ë˜ê³  ì§€ì†ì ì¸ ìƒí˜¸ì‘ìš©** ì œê³µ  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. ì‘ì—… ì§€í–¥í˜• ì—ì´ì „íŠ¸ (Task-Oriented Agents)\n",
    "\n",
    "ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ëŠ”\n",
    "ë‹¨ê¸° ë©”ëª¨ë¦¬ëŠ” **ì´ì „ ë‹¨ê³„, í˜„ì¬ ì§„í–‰ ìƒí™©, ì „ì²´ ëª©í‘œ**ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤.  \n",
    "\n",
    "ì¥ê¸° ë©”ëª¨ë¦¬ëŠ” ì¦‰ì‹œ ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” **ì‚¬ìš©ì ê´€ë ¨ ë°ì´í„°**ë¥¼ ê²€ìƒ‰í•  ë•Œ ì¤‘ìš”í•˜ë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìì˜ íŠ¹ì • ì—…ë¬´ ì´ë ¥ì´ë‚˜ ê³¼ê±° í™˜ê²½ ì„¤ì •ì„ í™œìš©í•´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìë™ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ê°œì¸í™”ëœ ê²½í—˜ (Personalized Experiences)\n",
    "\n",
    "ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ëŠ” **ì¥ê¸° ë©”ëª¨ë¦¬**ë¥¼ í™œìš©í•´  \n",
    "ì‚¬ìš©ìì˜ **ì„ í˜¸ë„, ê³¼ê±° í–‰ë™, ê°œì¸ ì •ë³´** ë“±ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•œë‹¤.  \n",
    "\n",
    "ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì‚¬ìš©ìì˜ ë§¥ë½ì— ë§ì¶° ì‘ë‹µê³¼ ì œì•ˆì„ ì¡°ì •í•˜ê³ ,  \n",
    "ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë”ìš± **ì ì‘í˜•(interactive adaptive)** ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. í•™ìŠµ ë° ì„±ëŠ¥ í–¥ìƒ (Learning and Improvement)\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” **ê³¼ê±° ìƒí˜¸ì‘ìš©ìœ¼ë¡œë¶€í„° í•™ìŠµ**í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.  \n",
    "- ì„±ê³µì ì¸ ì „ëµ(successful strategies)  \n",
    "- ì‹¤ìˆ˜(mistakes)  \n",
    "- ìƒˆë¡œìš´ ì •ë³´(new insights)  \n",
    "\n",
    "ì´ëŸ¬í•œ ê²½í—˜ì€ **ì¥ê¸° ë©”ëª¨ë¦¬**ì— ì €ì¥ë˜ì–´,  \n",
    "ë¯¸ë˜ì˜ ì ì‘ê³¼ ê°œì„ ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.  \n",
    "\n",
    "íŠ¹íˆ **ê°•í™”í•™ìŠµ(Reinforcement Learning)** ê¸°ë°˜ì˜ ì—ì´ì „íŠ¸ëŠ”  \n",
    "í•™ìŠµëœ ì „ëµê³¼ ì§€ì‹ì„ ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì—¬, ë°˜ë³µì  ê°œì„ ì„ ì´ëŒì–´ë‚¸ë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ì •ë³´ ê²€ìƒ‰ ë° RAG (Information Retrieval, RAG)\n",
    "\n",
    "ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì—ì´ì „íŠ¸ëŠ” ì¢…ì¢…  \n",
    "ìì‹ ì˜ ì¥ê¸° ë©”ëª¨ë¦¬ì— í•´ë‹¹í•˜ëŠ” **ì§€ì‹ ê¸°ë°˜(knowledge base)**ì— ì ‘ê·¼í•œë‹¤.  \n",
    "\n",
    "ì´ êµ¬ì¡°ëŠ” **Retrieval-Augmented Generation (RAG)** í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„ë˜ë©°,  \n",
    "ì—ì´ì „íŠ¸ëŠ” ê´€ë ¨ ë¬¸ì„œë‚˜ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬  \n",
    "ë³´ë‹¤ ì •í™•í•˜ê³  ë§¥ë½ì ìœ¼ë¡œ í’ë¶€í•œ ì‘ë‹µì„ ìƒì„±í•œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ììœ¨ ì‹œìŠ¤í…œ (Autonomous Systems)\n",
    "\n",
    "ë¡œë´‡ì´ë‚˜ ììœ¨ì£¼í–‰ ìë™ì°¨ì™€ ê°™ì€ ì‹œìŠ¤í…œì€ ë‹¤ìŒì„ ìœ„í•´ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•œë‹¤:\n",
    "- **ì§€ë„(map)**  \n",
    "- **ê²½ë¡œ(route)**  \n",
    "- **ê°ì²´ ìœ„ì¹˜(object locations)**  \n",
    "- **í•™ìŠµëœ í–‰ë™ íŒ¨í„´(learned behaviors)**  \n",
    "\n",
    "ì´ ê²½ìš°, **ë‹¨ê¸° ë©”ëª¨ë¦¬**ëŠ” ì¦‰ê°ì ì¸ ì£¼ë³€ í™˜ê²½ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³ ,  \n",
    "**ì¥ê¸° ë©”ëª¨ë¦¬**ëŠ” ì¼ë°˜ì ì¸ í™˜ê²½ ì§€ì‹ì´ë‚˜ ëˆ„ì ëœ ê²½í—˜ì„ ì €ì¥í•œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ê²°ë¡ \n",
    "\n",
    "ë©”ëª¨ë¦¬ëŠ” ì—ì´ì „íŠ¸ê°€ ë‹¤ìŒì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œì´ë‹¤:\n",
    "- **ì—­ì‚¬(history)**ë¥¼ ìœ ì§€í•˜ê³   \n",
    "- **í•™ìŠµ(learning)**ì„ ì§€ì†í•˜ë©°  \n",
    "- **ìƒí˜¸ì‘ìš©ì„ ê°œì¸í™”(personalize)**í•˜ê³   \n",
    "- **ë³µì¡í•˜ê³  ì‹œê°„ ì˜ì¡´ì ì¸ ë¬¸ì œ(complex, time-dependent problems)**ë¥¼ ê´€ë¦¬í•œë‹¤.\n",
    "\n",
    "ì¦‰, íš¨ê³¼ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” **ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ì˜ í•µì‹¬ í† ëŒ€**ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02d16b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi there! How can I assist you today?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_message(HumanMessage(content=\"Hello!\"))\n",
    "history.add_message(AIMessage(content=\"Hi there! How can I assist you today?\"))\n",
    "\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hello!\\nAI: Hi there! How can I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/rnm2z36939g8qznfcdykwkzh0000gn/T/ipykernel_20550/2163870243.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello!\"},\n",
    "    {\"output\": \"Hi there! How can I assist you today?\"}\n",
    ")\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2280319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "839b65e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='6398303b-ef9e-4576-8ef2-74b848c03171'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 19, 'total_tokens': 32, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-B80pAOiMbt2s3W5tedXig5rqZZm6PDZ3', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4022d961-87a9-4fef-9523-d38086276092-0', usage_metadata={'input_tokens': 19, 'output_tokens': 13, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd465bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='6398303b-ef9e-4576-8ef2-74b848c03171'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 19, 'total_tokens': 32, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-B80pAOiMbt2s3W5tedXig5rqZZm6PDZ3', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4022d961-87a9-4fef-9523-d38086276092-0', usage_metadata={'input_tokens': 19, 'output_tokens': 13, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='578a7d90-f313-4f5c-97b9-97512f47deee'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 48, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-XFGYyvudnioKgqgM2mhIpEkOr3Xwhb47', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9d6796d0-930e-4eb3-8cc5-494492674578-0', usage_metadata={'input_tokens': 48, 'output_tokens': 13, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]}, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a31ad525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='6398303b-ef9e-4576-8ef2-74b848c03171'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 19, 'total_tokens': 32, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-B80pAOiMbt2s3W5tedXig5rqZZm6PDZ3', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4022d961-87a9-4fef-9523-d38086276092-0', usage_metadata={'input_tokens': 19, 'output_tokens': 13, 'total_tokens': 32, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='578a7d90-f313-4f5c-97b9-97512f47deee'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 48, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-XFGYyvudnioKgqgM2mhIpEkOr3Xwhb47', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9d6796d0-930e-4eb3-8cc5-494492674578-0', usage_metadata={'input_tokens': 48, 'output_tokens': 13, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='f0dd87d4-2852-4d2d-bdbf-11e480e81148'),\n",
       "  AIMessage(content='Your name is Bob. ğŸ˜Š How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 75, 'total_tokens': 90, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-wJP7XlkZD91dnh0jrXOV9CE6oh5rMWcq', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--bacade1f-5848-42ec-a799-ef5b7f28bab5-0', usage_metadata={'input_tokens': 75, 'output_tokens': 15, 'total_tokens': 90, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]}, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abf779e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='3695d991-92c4-4832-b3cb-d8f8ecf6f18c'),\n",
       "  AIMessage(content=\"I don't know your name. You may need to tell me your name or provide more context about who you are. If you're asking about my name, I am Qwen, a large language model developed by Alibaba Cloud.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 17, 'total_tokens': 64, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-Aj1kgbiaADXx2UI9cbLXLYum1VIzJJr4', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--fce4db25-788e-4437-a709-5100fd3f0bd8-0', usage_metadata={'input_tokens': 17, 'output_tokens': 47, 'total_tokens': 64, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "             {\"configurable\": {\"thread_id\": \"2\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ed9869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d7876ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "summary_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,\n",
    "    extra_body={\"n_predict\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bf8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=summary_llm,\n",
    "            max_tokens_before_summary=200,\n",
    "            messages_to_keep=5,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73fed3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='a1ea475b-6270-42d1-a792-9ddb5f9d14e9'),\n",
       "  AIMessage(content=\"Hello, Bob! It's nice to meet you. How can I assist you today? ğŸ˜Š\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 19, 'total_tokens': 40, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-zgLfHQwTADDeckgCVucmLLIFrQnJjdAj', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--50bad4cc-3f03-4abc-86b2-14ffea077111-0', usage_metadata={'input_tokens': 19, 'output_tokens': 21, 'total_tokens': 40, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test_thread_1\"}}\n",
    "\n",
    "response1 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    config\n",
    ")\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72765160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 -> content='Sure! Here\\'s a generic question about Bob and memory:\\n\\n**\"Bob, do you remember how we first met?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 61, 'total_tokens': 89, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-uI3LGmFrWWrZmbbhxcqd0gp7SNJBuuVK', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--6472fb2c-630d-4b2f-b673-c0b5688901fe-0' usage_metadata={'input_tokens': 61, 'output_tokens': 28, 'total_tokens': 89, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 2 -> content='Of course! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember the last time we talked?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 110, 'total_tokens': 140, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-zfF5PHpB41obtD5UbXdPhn345fOjN8IV', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--55e3470d-d717-498d-949f-08c262aff833-0' usage_metadata={'input_tokens': 110, 'output_tokens': 30, 'total_tokens': 140, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 3 -> content='Absolutely! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember anything from our last conversation?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 161, 'total_tokens': 190, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-HjaspRASY9QaLmfJeOouUMN961BQTHhR', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--ee2df372-b07a-4ad3-b78f-b7a0c9144ae3-0' usage_metadata={'input_tokens': 161, 'output_tokens': 29, 'total_tokens': 190, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 4 -> content='Sure! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember where we met last time?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 176, 'total_tokens': 205, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-hbj70hbNaJJfKiM7LrNxATdiyymu0smt', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--73a79b4f-a495-4504-8c7e-c46088dfdc51-0' usage_metadata={'input_tokens': 176, 'output_tokens': 29, 'total_tokens': 205, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 5 -> content='Of course! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember what we were discussing earlier?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 219, 'total_tokens': 249, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-UxmF2lnBZtjbQft1J6Z1R8ASb5oR893B', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--ad43ff9f-7a6c-4888-bfb4-0bd3eb655fb8-0' usage_metadata={'input_tokens': 219, 'output_tokens': 30, 'total_tokens': 249, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 6 -> content='Absolutely! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember the date we first met?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 263, 'total_tokens': 292, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-cBj2UPX2F2kZGjr82O9oIPYrloCm8Vk8', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--520e7082-8c74-4068-89c8-c59df15de497-0' usage_metadata={'input_tokens': 263, 'output_tokens': 29, 'total_tokens': 292, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 7 -> content='Certainly! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember any details from our first conversation?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 316, 'total_tokens': 346, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-AekWTGoIHUiwPqyChXiGV4LGWo0cYQ6o', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--67b7eb36-f28e-4553-bf64-35a0bdf7281d-0' usage_metadata={'input_tokens': 316, 'output_tokens': 30, 'total_tokens': 346, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 8 -> content='Of course! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember anything about our initial meeting?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 350, 'total_tokens': 380, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-mXWc1A9UIaxzcrdcmf6KrYoF0Fww5t3N', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--d0059157-a0e9-4b77-902f-576ab88f63ca-0' usage_metadata={'input_tokens': 350, 'output_tokens': 30, 'total_tokens': 380, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Turn 9 -> content='Sure! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember who we were talking to during our first meeting?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 393, 'total_tokens': 426, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-AMchqqgVdLtpLywyVd7BTS162ZuRnRGn', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1f1fb1dc-3689-4d6f-b14c-14a8986f7a7a-0' usage_metadata={'input_tokens': 393, 'output_tokens': 33, 'total_tokens': 426, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    user_msg = f\"Turn {i}: ask something generic about Bob and memory.\"\n",
    "    response = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_msg}]},\n",
    "        config\n",
    "    )\n",
    "    print(f\"Turn {i} ->\", response[\"messages\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2342dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final check â€“ Whatâ€™s my name?: content='Your name is **Bob**. ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 443, 'total_tokens': 453, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-TJMrxJSD9iOtAs1JsjW2rEunfn3nQeIx', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--66a7595a-552f-4b7e-bc35-4bc9a8967303-0' usage_metadata={'input_tokens': 443, 'output_tokens': 10, 'total_tokens': 453, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "response_final = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Whatâ€™s my name?\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Final check â€“ Whatâ€™s my name?:\", response_final[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5081c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\"messages\": []}, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39465d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned keys: dict_keys(['messages'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Returned keys:\", result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "515e7356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages history length: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Messages history length:\", len(result.get(\"messages\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbac624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is a summary of the conversation to date:\\n\\nHi! My name is Bob.  \\nSure! Here\\'s a generic question about Bob and memory:  \\n**\"Bob, do you remember how we first met?\"** ğŸ˜Š  \\nTurn 2: ask something generic about Bob and memory.  \\nOf course! Here\\'s another generic question about Bob and memory:  \\n**\"Bob, do you remember the last time we talked?\"** ğŸ˜Š  \\nTurn 3: ask something generic about Bob and memory.  \\nAbsolutely! Here\\'s another generic question about the memory:  \\n**\"Bob, do you remember anything from our last conversation?\"** ğŸ˜Š  \\nTurn 4: ask something generic about Bob and memory.  \\nSure! Here\\'s another generic question about Bob and memory:  \\n**\"Bob, do you remember where we met last time?\"** ğŸ˜Š  \\nTurn 5: ask something generic about Bob and memory.  \\nOf course! Here\\'s another generic question about Bob and memory:  \\n**\"Bob, do you remember what we were discussing earlier?\"** ğŸ˜Š  \\nTurn 6: ask something generic about Bob and memory.  \\nAbsolutely! Here\\'s another generic question about Bob and memory:  \\n**\"Bob, do you remember the date we first met?\"** ğŸ˜Š  \\nTurn 7: ask something generic about Bob and memory.  \\nCertainly! Here\\'s another generic question about Bob and memory:  \\n**\"Bob, do you remember any details from our first conversation?\"** ğŸ˜Š  \\nTurn 8: ask something generic about Bob and memory.' additional_kwargs={} response_metadata={} id='e2459c52-f98f-4310-9089-697479f717ba'\n",
      "content='Of course! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember anything about our initial meeting?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 350, 'total_tokens': 380, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-mXWc1A9UIaxzcrdcmf6KrYoF0Fww5t3N', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--d0059157-a0e9-4b77-902f-576ab88f63ca-0' usage_metadata={'input_tokens': 350, 'output_tokens': 30, 'total_tokens': 380, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='Turn 9: ask something generic about Bob and memory.' additional_kwargs={} response_metadata={} id='9fda8983-f61f-4c29-a8d9-d9ab3e330da0'\n",
      "content='Sure! Here\\'s another generic question about Bob and memory:\\n\\n**\"Bob, do you remember who we were talking to during our first meeting?\"** ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 393, 'total_tokens': 426, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-AMchqqgVdLtpLywyVd7BTS162ZuRnRGn', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1f1fb1dc-3689-4d6f-b14c-14a8986f7a7a-0' usage_metadata={'input_tokens': 393, 'output_tokens': 33, 'total_tokens': 426, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='Whatâ€™s my name?' additional_kwargs={} response_metadata={} id='75bc2b5d-c7a2-4fe3-94bc-db47c6a751bd'\n",
      "content='Your name is **Bob**. ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 443, 'total_tokens': 453, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-TJMrxJSD9iOtAs1JsjW2rEunfn3nQeIx', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--66a7595a-552f-4b7e-bc35-4bc9a8967303-0' usage_metadata={'input_tokens': 443, 'output_tokens': 10, 'total_tokens': 453, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 438, 'total_tokens': 439, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-tqLqo6gEM7lu8cVP7xc5Vu5IYYkI2CN3', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--6d3104d1-6db0-4adb-abed-74e179e5ee04-0' usage_metadata={'input_tokens': 438, 'output_tokens': 1, 'total_tokens': 439, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "for msg in result.get(\"messages\", []):\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c35d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "191dbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fef36b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def simple_agent(state: State) -> dict:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a concise assistant. Remember user's name if given.\"),\n",
    "        MessagesPlaceholder(\"messages\"),  # â† state[\"messages\"]ê°€ ê·¸ëŒ€ë¡œ ë“¤ì–´ê°„ë‹¤\n",
    "    ])\n",
    "    chain = prompt | llm  # Runnable\n",
    "\n",
    "    ai_msg = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a6dd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"agent\", simple_agent)\n",
    "g.set_entry_point(\"agent\")\n",
    "g.add_edge(\"agent\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = g.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "670f2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Same thread] Your name is Bob. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "cfg_same = {\"configurable\": {\"thread_id\": \"u1-thread\"}}\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"hi! I'm Bob\")]}, config=cfg_same)\n",
    "out = graph.invoke({\"messages\": [HumanMessage(content=\"what's my name?\")]}, config=cfg_same)\n",
    "print(\"[Same thread]\", out[\"messages\"][-1].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df0d383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='a4c14297-2bdf-4ba1-9648-1b9708af9cb3'),\n",
       "  AIMessage(content=\"I don't know your name. Would you like me to ask you?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 35, 'total_tokens': 51, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-Gq6KUtkcjYrKqhCpwEaiAgTgAKF1OQXF', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--abbcf952-c254-46b5-aa5b-93f45b3d5fe2-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_other = {\"configurable\": {\"thread_id\": \"u2-thread\"}}\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"what's my name?\")]}, config=cfg_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb2b84",
   "metadata": {},
   "source": [
    "## State: The Session's Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32c4c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import TypedDict, Annotated, Any, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.messages import (\n",
    "    AnyMessage, HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7b81652",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3886aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_kv(left: Dict[str, Any] | None, right: Dict[str, Any] | None) -> Dict[str, Any]:\n",
    "    left = left or {}\n",
    "    right = right or {}\n",
    "    # ì–•ì€ ë³‘í•©(shallow merge). í•„ìš”í•œ ê²½ìš° prefix(app:/user:/temp:) ê·œì¹™ì„ ì—¬ê¸°ì„œ ë‹¤ë£¸.\n",
    "    merged = {**left, **right}\n",
    "    # temp: í”„ë¦¬í”½ìŠ¤ëŠ” í˜„ì¬ í„´ ì´í›„ ë³´ì¡´í•˜ì§€ ì•Šë„ë¡ ì¦‰ì‹œ ì œê±°í•˜ëŠ” ì˜ˆ (ì„ íƒ)\n",
    "    merged = {k: v for k, v in merged.items() if not k.startswith(\"temp:\")}\n",
    "    return merged\n",
    "\n",
    "class State(TypedDict):\n",
    "    # ëŒ€í™” ë¡œê·¸ ì±„ë„: add_messages ë¦¬ë“€ì„œë¡œ ì•ˆì „ ëˆ„ì /ì‚­ì œ\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    # ì„¸ì…˜ ì‘ì—… ë©”ëª¨(scratchpad): ADKì˜ session.state ê°œë…\n",
    "    kv: Annotated[Dict[str, Any], merge_kv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99be9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ë„ˆëŠ” ê°„ê²°í•˜ê²Œ ëŒ€ë‹µí•˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\\n\"\n",
    "     \"ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ì„¸ì…˜ ìŠ¤í¬ë˜ì¹˜íŒ¨ë“œ(kv)ë¥¼ ì°¸ê³ í•´ì„œ ì •í™•íˆ ë‹µí•´.\"),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "    (\"system\", \"ì°¸ê³  ê°€ëŠ¥í•œ ìŠ¤í¬ë˜ì¹˜íŒ¨ë“œ í‚¤: {kv_keys}\")\n",
    "])\n",
    "\n",
    "def agent_node(state: State) -> dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    kv = state.get(\"kv\", {})\n",
    "    kv_keys = \", \".join(kv.keys()) or \"(ì—†ìŒ)\"\n",
    "    ai: AIMessage = (agent_prompt | llm).invoke({\"messages\": messages, \"kv_keys\": kv_keys})\n",
    "    return {\"messages\": [ai]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a3a8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameInfo(BaseModel):\n",
    "    name: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ë°íŒ ì´ë¦„/í˜¸ì¹­. ì—†ê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë©´ null.\"\n",
    "    )\n",
    "\n",
    "name_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ë„ˆëŠ” 'ì´ë¦„ ì¶”ì¶œê¸°'ì•¼. ìµœê·¼ ì‚¬ìš©ì ë°œí™”ì—ì„œ **ì‚¬ìš©ìê°€ ìê¸°ì†Œê°œí•œ ì´ë¦„/í˜¸ì¹­**ë§Œ ì°¾ì•„.\\n\"\n",
    "     \"- í•œêµ­ì–´/ì˜ì–´/ë‹‰ë„¤ì„ ê°€ëŠ¥. ì˜ˆ) 'ë‚´ ì´ë¦„ì€ í™ê¸¸ë™', 'I'm Alice', 'ë‚œ ë¯¼ìˆ˜ì•¼', 'ì œ ì´ë¦„ì€ ìœ ë¦¬ì…ë‹ˆë‹¤'\\n\"\n",
    "     \"- ì¼ë°˜ ëª…ì‚¬/íšŒì‚¬ëª…/ì§í•¨/íƒ€ì¸ì˜ ì´ë¦„ì€ ì œì™¸.\\n\"\n",
    "     \"- í™•ì‹¤ì¹˜ ì•Šìœ¼ë©´ name=null ë¡œ ë°˜í™˜.\\n\"\n",
    "     \"ë°˜ë“œì‹œ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì‘ë‹µí•´.\"),\n",
    "    (\"user\", \"ìµœê·¼ ì‚¬ìš©ì ë°œí™”:\\n{text}\")\n",
    "])\n",
    "\n",
    "nameinfo_llm = llm.with_structured_output(NameInfo)\n",
    "name_chain_structured = (name_prompt | nameinfo_llm)\n",
    "\n",
    "def extract_name_node(state: State) -> dict:\n",
    "    last_user = next((m for m in reversed(state.get(\"messages\", [])) if m.type == \"human\"), None)\n",
    "    if not last_user:\n",
    "        return {}\n",
    "    try:\n",
    "        result: NameInfo = name_chain_structured.invoke({\"text\": last_user.content})\n",
    "        if result and result.name:\n",
    "            return {\"kv\": {\"user:name\": result.name}}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41f9c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FOR_SUMMARY = 48\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ í•µì‹¬ ì‚¬ì‹¤(ì´ë¦„, ê²°ì •, ì‘ì—… ìƒíƒœ)ì„ ë³´ì¡´í•˜ë©´ì„œ 4~6ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ìš”ì•½í•´ì¤˜.\"),\n",
    "    (\"user\", \"{history_text}\")\n",
    "])\n",
    "\n",
    "def summarize_node(state: State) -> dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if len(messages) <= MAX_FOR_SUMMARY:\n",
    "        return {}\n",
    "    cut = int(MAX_FOR_SUMMARY * 0.6) # ì•ë¶€ë¶„ 60% ìš”ì•½\n",
    "    old, recent = messages[:-cut], messages[-cut:]\n",
    "    old_text = \"\\n\".join(f\"{m.type}: {getattr(m, 'content', '')}\"[:2000] for m in old)\n",
    "    summary_ai: AIMessage = (summary_prompt | llm).invoke({\"history_text\": old_text})\n",
    "    summary_sys = SystemMessage(content=f\"[ëŒ€í™” ìš”ì•½]\\n{summary_ai.content}\")\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), summary_sys, *recent]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3968b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_RECENT_K = 24\n",
    "\n",
    "def trim_node(state: State) -> dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if len(messages) <= KEEP_RECENT_K:\n",
    "        return {}\n",
    "    keep = messages[-KEEP_RECENT_K:]\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *keep]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8bc28f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1288fbe50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(State)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"extract_name\", extract_name_node)\n",
    "graph.add_node(\"summarize\", summarize_node)\n",
    "graph.add_node(\"trim\", trim_node)\n",
    "\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_edge(\"agent\", \"extract_name\")\n",
    "graph.add_edge(\"extract_name\", \"summarize\")\n",
    "graph.add_edge(\"summarize\", \"trim\")\n",
    "graph.add_edge(\"trim\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "957a4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d634b72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='f51f889b-f5f2-4be6-b465-46f929d73cad'),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 100, 'total_tokens': 127, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-Ue64NovzHrdcno6eFNkFfkjNcTiETwiw', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--68df4475-4d6c-4830-8b97-531c6668e2c5-0', usage_metadata={'input_tokens': 100, 'output_tokens': 27, 'total_tokens': 127, 'input_token_details': {}, 'output_token_details': {}})],\n",
       " 'kv': {'user:name': 'í™ê¸¸ë™'}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\"configurable\": {\"thread_id\": \"sess-kr\"}}\n",
    "\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.\")]} , config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74950caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-kr] ë‹µë³€: í™ê¸¸ë™ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "out = graph.invoke({\"messages\": [HumanMessage(content=\"ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\")]} , config=cfg)\n",
    "print(\"[sess-kr] ë‹µë³€:\", out[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f06c160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'messages': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='f51f889b-f5f2-4be6-b465-46f929d73cad'), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 100, 'total_tokens': 127, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-Ue64NovzHrdcno6eFNkFfkjNcTiETwiw', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--68df4475-4d6c-4830-8b97-531c6668e2c5-0', usage_metadata={'input_tokens': 100, 'output_tokens': 27, 'total_tokens': 127, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content='ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', additional_kwargs={}, response_metadata={}, id='92e51db8-8458-45f7-8bb8-82c8629dd06b'), AIMessage(content='í™ê¸¸ë™ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 142, 'total_tokens': 148, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-m5e1M2qpvsBLNby04DWgskTtf1dmhVe2', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--07795ea8-e9dd-495f-be42-bd6a38229891-0', usage_metadata={'input_tokens': 142, 'output_tokens': 6, 'total_tokens': 148, 'input_token_details': {}, 'output_token_details': {}})], 'kv': {'user:name': 'í™ê¸¸ë™'}}, next=(), config={'configurable': {'thread_id': 'sess-kr', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2e81-8abc-63da-800a-bf6587bb36dd'}}, metadata={'source': 'loop', 'step': 10, 'parents': {}}, created_at='2025-11-16T12:30:55.059648+00:00', parent_config={'configurable': {'thread_id': 'sess-kr', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2e81-8abb-61a6-8009-a2d5073ecd4e'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "snap = graph.get_state(cfg)\n",
    "print(snap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d50a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ kv:  {'user:name': 'í™ê¸¸ë™'}\n",
      "ë©”ì‹œì§€ ìˆ˜: 4\n",
      "ìµœê·¼ ë©”ì‹œì§€ë“¤: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "print(\"í˜„ì¬ kv: \", snap.values['kv'])\n",
    "print(\"ë©”ì‹œì§€ ìˆ˜:\", len(snap.values[\"messages\"]))\n",
    "print(\"ìµœê·¼ ë©”ì‹œì§€ë“¤:\", [getattr(m, \"content\", None) for m in snap.values[\"messages\"][-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e046f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[íˆìŠ¤í† ë¦¬ íƒ€ì„ë¼ì¸]\n",
      " step=10: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=9: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=8: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=7: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=6: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?']\n",
      " step=5: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?']\n",
      " step=4: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?']\n",
      " step=3: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?']\n",
      " step=2: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?']\n",
      " step=1: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?']\n",
      " step=0: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=-1: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[íˆìŠ¤í† ë¦¬ íƒ€ì„ë¼ì¸]\")\n",
    "for s in graph.get_state_history(cfg):\n",
    "    step = s.metadata.get(\"step\")\n",
    "    msgs = [getattr(m, \"content\", None) for m in s.values[\"messages\"]]\n",
    "    print(f\" step={step}: {msgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8915d6",
   "metadata": {},
   "source": [
    "## user/app ë©”ëª¨ë¦¬ + ì„¸ì…˜/ìš”ì•½/íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7351c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Optional, Annotated, Dict, List\n",
    "from typing_extensions import TypedDict\n",
    "from operator import or_\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage, HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages, REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.runtime import Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67679897",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FOR_SUMMARY = 48\n",
    "MAX_KEEP = 24\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"ë„ˆëŠ” í•œêµ­ì–´ ë¹„ì„œì•¼. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì •ì¤‘í•˜ê²Œ.\\n\"\n",
    "    \"- ì‚¬ìš©ì ì •ë³´/ì•± ì •ì±…ì´ ì£¼ì–´ì§€ë©´ ì¡´ì¤‘í•´ì„œ ë‹µí•´.\\n\"\n",
    "    \"- ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a9a346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState(TypedDict):\n",
    "    # messagesëŠ” add_messages ë¦¬ë“€ì„œ(ëŒ€í™” ëˆ„ì )\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    # kvëŠ” ê°„ë‹¨ ë¨¸ì§€(ë”•ì…”ë„ˆë¦¬ í•©ì¹˜ê¸°)\n",
    "    kv: Annotated[Dict[str, Any], or_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85c245b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory_node(state: AppState, config, store: BaseStore):\n",
    "    user_id = (config.get(\"configurable\") or {}).get(\"user_id\", \"anonymous\")\n",
    "    app_id = (config.get(\"configurable\") or {}).get(\"app_id\", \"default_app\")\n",
    "\n",
    "    ns_user = (\"memories\", \"user\", user_id)\n",
    "    user_items = store.search(ns_user)\n",
    "    user_hints = []\n",
    "    for it in user_items:\n",
    "        if isinstance(it.value, dict):\n",
    "            if it.value.get(\"name\"):\n",
    "                user_hints.append(f\"ì‚¬ìš©ì ì´ë¦„: {it.value['name']}\")\n",
    "            if it.value.get(\"pref\"):\n",
    "                user_hints.append(f\"ì‚¬ìš©ì ì„ í˜¸: {it.value['pref']}\")\n",
    "\n",
    "    ns_app = (\"memories\", \"app\", app_id)\n",
    "    app_items = store.search(ns_app)\n",
    "    app_hints = []\n",
    "    for it in app_items:\n",
    "        if isinstance(it.value, dict) and it.value.get(\"policy\"):\n",
    "            app_hints.append(f\"ì•± ì •ì±…: {it.value['policy']}\")\n",
    "\n",
    "    memory_hints = \"\\n\".join([*user_hints, *app_hints]) or \"ì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ\"\n",
    "    return {\n",
    "        \"messages\": [SystemMessage(content=f\"[ì»¨í…ìŠ¤íŠ¸]\\n{memory_hints}\")],\n",
    "        \"kv\": {\"ctx:last_memory_hint\": memory_hints, \"ctx:last_sys\": SYSTEM_PROMPT}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "265f2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: AppState):\n",
    "    agent_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{system_text}\"),\n",
    "        (\"system\", \"{memory_hints}\"),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "    ])\n",
    "\n",
    "    system_text = state[\"kv\"].get(\"ctx:last_sys\", SYSTEM_PROMPT)\n",
    "    memory_hints = state[\"kv\"].get(\"ctx:last_memory_hint\", \"ì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ\")\n",
    "    prompt_msgs = agent_prompt.format_messages(\n",
    "        system_text=system_text,\n",
    "        memory_hints=memory_hints,\n",
    "        messages=state[\"messages\"]\n",
    "    )\n",
    "    ai: AIMessage = llm.invoke(prompt_msgs)\n",
    "    return {\"messages\": [ai]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd294330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameInfo(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "\n",
    "def extract_and_store_name_node(state: AppState, config, store: BaseStore):\n",
    "    # (ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸/êµ¬ì¡°ì  LLMì„ \"ë…¸ë“œ ê·¼ì²˜\"ì—ì„œ ì •ì˜)\n",
    "    name_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"ë„ˆëŠ” 'ì´ë¦„ ì¶”ì¶œê¸°'ì•¼. ìµœê·¼ ì‚¬ìš©ì ë°œí™”ì—ì„œ ìê¸°ì†Œê°œí•œ ì´ë¦„ë§Œ ì°¾ì•„.\\n\"\n",
    "         \"- í™•ì‹¤ì¹˜ ì•Šìœ¼ë©´ name=null ë¡œ ë°˜í™˜.\\n\"\n",
    "         \"ë°˜ë“œì‹œ JSON ìŠ¤í‚¤ë§ˆ(NameInfo)ë¡œë§Œ ì‘ë‹µí•´.\"),\n",
    "        (\"user\", \"ìµœê·¼ ì‚¬ìš©ì ë°œí™”:\\n{text}\")\n",
    "    ])\n",
    "    structured_llm = llm.with_structured_output(NameInfo)\n",
    "    name_chain = (name_prompt | structured_llm)\n",
    "\n",
    "    last_user = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
    "    if not last_user:\n",
    "        return {}\n",
    "\n",
    "    info: NameInfo = name_chain.invoke({\"text\": last_user.content})\n",
    "    if info and info.name:\n",
    "        user_id = (config.get(\"configurable\") or {}).get(\"user_id\", \"anonymous\")\n",
    "        ns_user = (\"memories\", \"user\", user_id)\n",
    "        store.put(ns_user, key=\"name\", value={\"name\": info.name}) # ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        return {\"app_memory\": {\"user:name\": info.name}}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_node(state: AppState) -> dict:\n",
    "    if len(state.get(\"messages\", [])) <= MAX_FOR_SUMMARY:\n",
    "        return {}\n",
    "\n",
    "    summ_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "            \"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ í•µì‹¬ ì‚¬ì‹¤(ì´ë¦„, ê²°ì •, ì‘ì—… ìƒíƒœ)ì„ ë³´ì¡´í•˜ë©´ì„œ 4~6ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ìš”ì•½í•´ì¤˜.\"),\n",
    "        (\"user\", \"{history_text}\")\n",
    "    ])\n",
    "\n",
    "    cut = int(MAX_FOR_SUMMARY * 0.6) # ì•ë¶€ë¶„ 60% ìš”ì•½\n",
    "    old, recent = state[\"messages\"][:-cut], state[\"messages\"][-cut:]\n",
    "    old_text = \"\\n\".join(f\"{m.type}: {getattr(m, 'content', '')}\"[:2000] for m in old)\n",
    "\n",
    "    summary_ai: AIMessage = (summ_prompt | llm).invoke({\"history_text\": old_text})\n",
    "    summary_sys = SystemMessage(content=f\"[ëŒ€í™” ìš”ì•½]\\n{summary_ai.content}\")\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), summary_sys, *recent]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a074782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_node(state: AppState) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) <= MAX_KEEP:\n",
    "        return {}\n",
    "    keep = messages[-MAX_KEEP:]\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *keep]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9184059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_graph = StateGraph(AppState)\n",
    "build_graph.add_node(\"load_memory\", load_memory_node)\n",
    "build_graph.add_node(\"agent\", agent_node)\n",
    "build_graph.add_node(\"name\", extract_and_store_name_node)\n",
    "build_graph.add_node(\"summarize\", summarize_node)\n",
    "build_graph.add_node(\"trim\", trim_node)\n",
    "\n",
    "build_graph.add_edge(START, \"load_memory\")\n",
    "build_graph.add_edge(\"load_memory\", \"agent\")\n",
    "build_graph.add_edge(\"agent\", \"name\")\n",
    "build_graph.add_edge(\"name\", \"summarize\")\n",
    "build_graph.add_edge(\"summarize\", \"trim\")\n",
    "build_graph.add_edge(\"trim\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "store = InMemoryStore()\n",
    "\n",
    "graph = build_graph.compile(checkpointer=checkpointer, store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "598d655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"configurable\": {\"thread_id\": \"sess-kr\", \"user_id\": \"user-123\", \"app_id\": \"app-xyz\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "228a356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-kr] ë‹µë³€: ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "out = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.\")]},\n",
    "    config=cfg\n",
    ")\n",
    "print(\"[sess-kr] ë‹µë³€:\", out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6bd6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-kr] ë‹µë³€: ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "out = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"ë‚´ ì´ë¦„ ê¸°ì–µí•´?\")]},\n",
    "    config=cfg\n",
    ")\n",
    "print(\"[sess-kr] ë‹µë³€:\", out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "268317e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-en] ë‹µë³€: ë‹¹ì‹ ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "cfg2  = {\"configurable\": {\"thread_id\": \"sess-en\", \"user_id\": \"user-123\", \"app_id\": \"app-xyz\"}}\n",
    "out = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"ë‚´ê°€ ëˆ„êµ¬ì˜€ì§€?\")]},\n",
    "    config=cfg2\n",
    ")\n",
    "print(\"[sess-en] ë‹µë³€:\", out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff029cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-kr] ë‹µë³€: ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.\n"
     ]
    }
   ],
   "source": [
    "store.put((\"memories\", \"app\", \"app-xyz\"), \"policy\", {\"policy\": \"ë°˜ë§ ê¸ˆì§€\"})\n",
    "out = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?\")]},\n",
    "    config=cfg\n",
    ")\n",
    "print(\"[sess-kr] ë‹µë³€:\", out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16f07713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[sess-kr] í˜„ì¬ kv:  {'ctx:last_memory_hint': 'ì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ctx:last_sys': 'ë„ˆëŠ” í•œêµ­ì–´ ë¹„ì„œì•¼. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì •ì¤‘í•˜ê²Œ.\\n- ì‚¬ìš©ì ì •ë³´/ì•± ì •ì±…ì´ ì£¼ì–´ì§€ë©´ ì¡´ì¤‘í•´ì„œ ë‹µí•´.\\n- ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´.'}\n",
      "ë©”ì‹œì§€ ìˆ˜: 12\n",
      "ìµœê·¼ ë©”ì‹œì§€ë“¤: ['[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.']\n"
     ]
    }
   ],
   "source": [
    "snap = graph.get_state(cfg)\n",
    "print(\"\\n[sess-kr] í˜„ì¬ kv: \", snap.values['kv'])\n",
    "print(\"ë©”ì‹œì§€ ìˆ˜:\", len(snap.values[\"messages\"]))\n",
    "print(\"ìµœê·¼ ë©”ì‹œì§€ë“¤:\", [getattr(m, \"content\", None) for m in snap.values[\"messages\"][-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dab6ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[íˆìŠ¤í† ë¦¬ íƒ€ì„ë¼ì¸]\n",
      " step=26: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.']\n",
      " step=25: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.']\n",
      " step=24: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.']\n",
      " step=23: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë°˜ë§ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™ê¸¸ë™ë‹˜.']\n",
      " step=22: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™\\nì•± ì •ì±…: ë°˜ë§ ê¸ˆì§€']\n",
      " step=21: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?']\n",
      " step=20: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.']\n",
      " step=19: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.']\n",
      " step=18: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.']\n",
      " step=17: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.']\n",
      " step=16: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ë°˜ë§ë¡œ ëŒ€ë‹µí•´ë„ ë¼ìš”. í™ê¸¸ë™ë‹˜.']\n",
      " step=15: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™']\n",
      " step=14: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', 'ë‚˜ë‘ ë°˜ë§í•´ë„ ë¼?']\n",
      " step=13: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=12: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=11: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=10: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=9: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™', 'ë„¤, ê¸°ì–µí•´ìš”. ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=8: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?', '[ì»¨í…ìŠ¤íŠ¸]\\nì‚¬ìš©ì ì´ë¦„: í™ê¸¸ë™']\n",
      " step=7: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', 'ë‚´ ì´ë¦„ ê¸°ì–µí•´?']\n",
      " step=6: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.']\n",
      " step=5: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.']\n",
      " step=4: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.']\n",
      " step=3: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.']\n",
      " step=2: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ', 'ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.']\n",
      " step=1: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.', '[ì»¨í…ìŠ¤íŠ¸]\\nì¶”ê°€ ë©”ëª¨ë¦¬ ì—†ìŒ']\n",
      " step=0: ['ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤.']\n",
      " step=-1: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[íˆìŠ¤í† ë¦¬ íƒ€ì„ë¼ì¸]\")\n",
    "for s in graph.get_state_history(cfg):\n",
    "    step = s.metadata.get(\"step\")\n",
    "    msgs = [getattr(m, \"content\", None) for m in s.values[\"messages\"]]\n",
    "    print(f\" step={step}: {msgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba648fa3",
   "metadata": {},
   "source": [
    "## ë©”ëª¨ë¦¬ ì‹œë§¨í‹± ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d0f715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import requests\n",
    "from typing import Any, Dict, List, Optional, Sequence, Annotated\n",
    "from operator import or_\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.base import BaseStore, SearchItem\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7bc37da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ë²¡í„° ê¸¸ì´: 768\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,  # llama-server endpoint\n",
    "    api_key=API_KEY,  # llama-serverëŠ” API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í•„ë“œëŠ” í•„ìˆ˜\n",
    "    model=MODEL_NAME,  # ëª¨ë¸ ì´ë¦„\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "EMBED_BASE_URL = os.getenv(\"EMBED_BASE_URL\")\n",
    "\n",
    "def _post_json(url: str, payload: dict) -> dict:\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def llama_server_embeddings(texts: Sequence[str]) -> List[List[float]]:\n",
    "    url = f\"{EMBED_BASE_URL}/v1/embeddings\"\n",
    "    try:\n",
    "        data = _post_json(url, {\"model\": \"embedding\", \"input\": list(texts)})\n",
    "        return [item[\"embedding\"] for item in data[\"data\"]]\n",
    "    except requests.HTTPError as e:\n",
    "        if e.response is not None and e.response.status_code in (404, 405):\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "\n",
    "vec = llama_server_embeddings([\"test\"])\n",
    "print(\"ì„ë² ë”© ë²¡í„° ê¸¸ì´:\", len(vec[0]) if vec else \"ì„ë² ë”© ì§€ì› ì•ˆí•¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0dbf881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    kv: Annotated[Dict[str, Any], or_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98da8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact(BaseModel):\n",
    "    text: str = Field(description=\"ì‚¬ìš©ì ë°œí™”ì—ì„œ ì¶”ì¶œí•œ ë‹¨ë¬¸ ì‚¬ì‹¤(ì„ í˜¸/í”„ë¡œí•„/ì•½ì† ë“±)\")\n",
    "    tags: Optional[List[str]] = Field(default=None, description=\"ì‚¬ì‹¤ ë¶„ë¥˜ íƒœê·¸ë“¤\")\n",
    "\n",
    "class Facts(BaseModel):\n",
    "    facts: List[Fact] = Field(default_factory=list, description=\"ì¶”ì¶œëœ ì‚¬ì‹¤ ëª©ë¡\")\n",
    "\n",
    "facts_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"ë„ˆëŠ” 'ì‚¬ì‹¤ ì¶”ì¶œê¸°'ì•¼.\\n\"\n",
    "     \"ìµœê·¼ ì‚¬ìš©ì ë°œí™”ì—ì„œ ì¬ì‚¬ìš© ê°€ì¹˜ê°€ ìˆëŠ” ì‚¬ì‹¤(ì„ í˜¸, í”„ë¡œí•„, ì•½ì†, ì œì•½ ë“±)ì„ 1~4ê°œ ì¶”ì¶œí•´.\\n\"\n",
    "     \"ëª¨í˜¸í•˜ë©´ facts=[]ë¡œ.\\n\"\n",
    "     \"ë°˜ë“œì‹œ JSON ìŠ¤í‚¤ë§ˆ(Facts)ë¡œë§Œ ì‘ë‹µí•´.\"),\n",
    "    (\"user\", \"ìµœê·¼ ì‚¬ìš©ì ë°œí™”:\\n{text}\")\n",
    "])\n",
    "\n",
    "def extract_facts(text: str) -> List[Fact]:\n",
    "    structured_llm = llm.with_structured_output(Facts)\n",
    "    fact_chain = (facts_prompt | structured_llm)\n",
    "    return fact_chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "367ceefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_facts_node(state: AppState, config: RunnableConfig, *, store: BaseStore):\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    user_id = cfg.get(\"user_id\", \"anon\")\n",
    "    ns = (\"memories\", \"user\", user_id, \"facts\")\n",
    "\n",
    "    last_user = next((m for m in reversed(state.get(\"messages\", [])) if isinstance(m, HumanMessage)), None)\n",
    "    if not last_user:\n",
    "        return {}\n",
    "    facts = extract_facts(last_user.content)\n",
    "    now = int(time.time())\n",
    "    for idx, f in enumerate(facts.facts):\n",
    "        key = f\"{now}-{idx}-{uuid.uuid4().hex[:8]}\"\n",
    "        store.put(ns, key, {\"text\": f.text, \"tags\": f.tags or [], \"source\": \"user\"})\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b554b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_memory_node(state: AppState, config: RunnableConfig, *, store: BaseStore):\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    user_id = cfg.get(\"user_id\", \"anonymous\")\n",
    "    ns_prefix = (\"memories\", \"user\", user_id)\n",
    "\n",
    "    query = (state[\"messages\"][-1].content if state.get(\"messages\") else \"\")\n",
    "    if not query:\n",
    "        return {}\n",
    "\n",
    "    hits: List[SearchItem] = store.search(ns_prefix, query=query, filter={\"source\": \"user\"}, limit=5)\n",
    "    if not hits:\n",
    "        return {}\n",
    "\n",
    "    bullets = \"\\n\".join(f\"- ){h.score:.2f}) {h.value.get('text', '')}\" for h in hits)\n",
    "    ctx = \"[ê²€ìƒ‰ëœ ì‚¬ìš©ì ë©”ëª¨ë¦¬]\\n\" + bullets\n",
    "    return {\"messages\": [SystemMessage(content=ctx)], \"kv\": {\"last_hits\": [h.value for h in hits]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27b03389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: AppState):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ë„ˆëŠ” í•œêµ­ì–´ ë¹„ì„œì•¼. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸(ìˆìœ¼ë©´)ì™€ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•´ ì •í™•í•˜ê³  ê°„ê²°íˆ ë‹µí•´.\"),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "    ])\n",
    "    messages = prompt.format_messages(messages=state[\"messages\"])\n",
    "    ai: AIMessage = llm.invoke(messages)\n",
    "    return {\"messages\": [ai]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1296215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 768\n",
    "store: BaseStore = InMemoryStore(index={\"embed\": llama_server_embeddings, \"dims\": dims})\n",
    "\n",
    "build_graph = StateGraph(AppState)\n",
    "build_graph.add_node(\"save_facts\", save_facts_node)\n",
    "build_graph.add_node(\"search_memory\", search_memory_node)\n",
    "build_graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "build_graph.add_edge(START, \"save_facts\")\n",
    "build_graph.add_edge(\"save_facts\", \"search_memory\")\n",
    "build_graph.add_edge(\"search_memory\", \"agent\")\n",
    "build_graph.add_edge(\"agent\", END)\n",
    "\n",
    "graph = build_graph.compile(checkpointer=InMemorySaver(), store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "af068ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ë‚œ ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³  ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´.', additional_kwargs={}, response_metadata={}, id='62ff52bb-6e40-4ee6-a044-8d548cd0187e'),\n",
       "  SystemMessage(content='[ê²€ìƒ‰ëœ ì‚¬ìš©ì ë©”ëª¨ë¦¬]\\n- )0.85) ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´\\n- )0.81) ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³ ', additional_kwargs={}, response_metadata={}, id='97ebf154-531c-4cc7-b7c6-7d33d210b4db'),\n",
       "  AIMessage(content='ê·¸ë ‡êµ°ìš”! ë§¤ìš´ ìŒì‹ì€ í”¼í•˜ê³  ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•˜ì‹œë„¤ìš”. ê·¸ëŸ° ë¶„ìœ„ê¸°ì˜ ìŒì‹ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 126, 'total_tokens': 170, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-bJ2szVt5KMuGt4yN8ouIkh2DzreGeG4t', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--28d99783-806e-478e-af23-7a32d43df15c-0', usage_metadata={'input_tokens': 126, 'output_tokens': 44, 'total_tokens': 170, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ , ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´.', additional_kwargs={}, response_metadata={}, id='8adfb404-3ace-421c-a91c-3d84c4ae7636'),\n",
       "  SystemMessage(content='[ê²€ìƒ‰ëœ ì‚¬ìš©ì ë©”ëª¨ë¦¬]\\n- )0.84) ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´\\n- )0.82) ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ \\n- )0.70) ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´\\n- )0.64) ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³ ', additional_kwargs={}, response_metadata={}, id='d2eab702-8158-4040-9bef-6311e0332f7c'),\n",
       "  AIMessage(content='ì£¼ë§ì—ëŠ” ë“±ì‚°ì„ í•˜ì‹œê³ , ì‚°ë¯¸ê°€ ë‚˜ëŠ” ì»¤í”¼ë¥¼ ì¢‹ì•„í•˜ì‹œë„¤ìš”. ê·¸ëŸ° ì·¨í–¥ì— ë§ëŠ” ì¶”ì²œì„ ë“œë¦´ê²Œìš”.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 296, 'total_tokens': 336, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-vmDza5ZzFpalw1vRJx7DSI4koHklaoUZ', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--791a36af-822a-4192-bb2f-181eedeb2ae3-0', usage_metadata={'input_tokens': 296, 'output_tokens': 40, 'total_tokens': 336, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='ì—…ë¬´ì—ì„  íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´.', additional_kwargs={}, response_metadata={}, id='0e0ba4c4-2bf4-4938-823a-1e6f1f94693d'),\n",
       "  SystemMessage(content='[ê²€ìƒ‰ëœ ì‚¬ìš©ì ë©”ëª¨ë¦¬]\\n- )0.88) ì—…ë¬´ì—ì„  íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´.\\n- )0.60) ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ \\n- )0.59) ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´\\n- )0.58) ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´\\n- )0.57) ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³ ', additional_kwargs={}, response_metadata={}, id='a3e1f07f-8dca-4834-9772-8ed26dcbc029'),\n",
       "  AIMessage(content='ì—…ë¬´ì—ì„œëŠ” íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•˜ì‹œë„¤ìš”. ê·¸ëŸ° ê¸°ìˆ  ìŠ¤íƒì— ë§ëŠ” ì •ë³´ë¥¼ ë„ì™€ë“œë¦´ê²Œìš”.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 484, 'total_tokens': 523, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'Qwen_Qwen3-4B-Q4_K_M', 'system_fingerprint': 'b6800-0398752d', 'id': 'chatcmpl-kSBMKRSuFt268trTgwCvilYruQINE7oj', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--76ce9169-72f5-4c76-9217-003895660914-0', usage_metadata={'input_tokens': 484, 'output_tokens': 39, 'total_tokens': 523, 'input_token_details': {}, 'output_token_details': {}})],\n",
       " 'kv': {'last_hits': [{'text': 'ì—…ë¬´ì—ì„  íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´.',\n",
       "    'tags': ['ê¸°ìˆ  ìŠ¤íƒ', 'ì—…ë¬´ í™˜ê²½'],\n",
       "    'source': 'user'},\n",
       "   {'text': 'ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ ', 'tags': ['ì„ í˜¸', 'í”„ë¡œí•„'], 'source': 'user'},\n",
       "   {'text': 'ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´', 'tags': ['ì„ í˜¸'], 'source': 'user'},\n",
       "   {'text': 'ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´', 'tags': ['ì„ í˜¸', 'í”„ë¡œí•„'], 'source': 'user'},\n",
       "   {'text': 'ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³ ', 'tags': ['ì„ í˜¸', 'ì œì•½'], 'source': 'user'}]}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\"configurable\": {\"thread_id\": \"t1\", \"user_id\": \"u-kr\"}}\n",
    "\n",
    "# (1) ì‚¬ìš©ì ì‚¬ì‹¤ì„ ë‚¨ê¸°ëŠ” ë°œí™”ë“¤\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"ë‚œ ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³  ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´.\")]}, config=cfg)\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ , ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´.\")]}, config=cfg)\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"ì—…ë¬´ì—ì„  íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´.\")]}, config=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c9dbb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì‘ë‹µ] ì‚°ë¯¸ê°€ ë‚˜ëŠ” ì»¤í”¼ë¥¼ ì¢‹ì•„í•˜ì‹œë˜ë°, ìš”ì¦˜ ì¹´í˜ì—ì„œ ì‚°ë¯¸ê°€ ê°•í•œ ì»¤í”¼ë¥¼ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”. ì˜ˆë¥¼ ë“¤ì–´, ë¼í…Œë‚˜ ì•„ë©”ë¦¬ì¹´ë…¸ ì¤‘ì—ì„œ ì‚°ë¯¸ê°€ ì˜ ëŠê»´ì§€ëŠ” ë ˆëª¬ ê·¸ë¼ì¸ë“œë‚˜ ì¹´í‘¸ì¹˜ë…¸ê°€ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n"
     ]
    }
   ],
   "source": [
    "# (2) ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆì˜ â†’ ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ ì—°ê´€ ë©”ëª¨ë¦¬ íšŒìˆ˜\n",
    "out = graph.invoke({\"messages\": [HumanMessage(content=\"ìš”ì¦˜ ì¹´í˜ì—ì„œ ì–´ë–¤ ì»¤í”¼ë¥¼ ì¶”ì²œí•´?\")]}, config=cfg)\n",
    "print(\"[ì‘ë‹µ]\", out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93db6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ ìƒìœ„]\n",
      "- ì»¤í”¼ëŠ” ì‚°ë¯¸ ìˆëŠ” í¸ì„ ì¢‹ì•„í•´\n",
      "- ì—…ë¬´ì—ì„  íŒŒì´ì¬ê³¼ íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´.\n",
      "- ì´íƒˆë¦¬ì•ˆ ìŒì‹ì„ ì¢‹ì•„í•´\n",
      "- ì£¼ë§ì—” ë³´í†µ ë“±ì‚° ê°€ê³ \n",
      "- ë§¤ìš´ ìŒì‹ì€ ì˜ ëª» ë¨¹ê³ \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ ìƒìœ„]\")\n",
    "for m in (out[\"kv\"].get(\"last_hits\") or []):\n",
    "    print(\"-\", m.get(\"text\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic-Design-Patterns-LanGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
